---
breadcrumb: VMware Tanzu RabbitMQ for VMs Documentation
title: Securing Inter-node Traffic with TLS
owner: London Services
---

This topic describes how to configure on-demand <%= vars.product_short %> to secure
communication between nodes in a RabbitMQ cluster.

##<a id="overview"></a> Overview

In <%= vars.product_short %>, nodes in a cluster communicate with other nodes
through Erlang distribution links. This is also true of RabbitMQ CLI tools, such
as <code>rabbitmqctl</code>, which establish connections to nodes in the cluster
through distribution links.

For on-demand service instances, you can secure this communication
between nodes in the cluster with <%= vars.product_short %> using mutual TLS
to provide authentication and encryption for the traffic.

For more information about clustering with TLS in RabbitMQ, see the
<a href="https://www.rabbitmq.com/clustering-ssl.html">RabbitMQ documentation</a>.

<p class="note">
  <strong>Note:</strong> Inter-node communication is independent of communication
  between your apps and RabbitMQ. You can secure inter-node traffic without modifying
  your apps to communicate over TLS.
</p>

To configure inter-node TLS for on-demand service instances:

1. [Configure Inter-node TLS](#configuration)
1. [Migrate to a Secure Inter-node Cluster](#migration)

##<a id="configuration"></a> Configure Inter-node TLS

To configure inter-node TLS in the <%= vars.product_short %> tile:

1. Click the <strong>Security for On-Demand Plans</strong> tab.
  <%= image_tag("/images/configure-oauth.png",
    :alt => "Screenshot of the Security for On-Demand Plans pane.
    The pane contains the **TLS Options** group of radio buttons,
    a checkbox to **Secure Inter-node communications with TLS on new Instances**,
    the **RabbitMQ TLS versions** group of checkboxes,
    the **OAuth Options** group of radio buttons, and a Save button.
    The example shows the **Not Configured** radio button selected for TLS Options,
    the Secure Inter-node communications with TLS checkbox unticked,
    TLS v1.3 and TLS v1.2 checkboxes selected for RabbitMQ TLS versions,
    and the **Optional** radio button selected for OAuth.") %>
  <a href="./images/configure-oauth.png" target="_blank" aria-hidden="true">Click here to view a larger version of the image above</a>

1. Select the **Secure Inter-node communications with TLS on new Instances** <%# has this checkbox name changed %>
checkbox.

1. Click <strong>Save</strong>.

1. Go back to **<%= vars.ops_manager %> Installation Dashboard** &gt; **Review Pending Changes**.

1. Click **Apply Changes** to apply the changes to the <%= vars.product_short %> tile.

After applying changes, newly created service instances communicate with mutual
TLS for inter-node traffic.

<p class="note warning">
  <strong>Warning:</strong> This option has no effect on existing service instances,
  and only affects instances created after running <strong>Apply Changes</strong>.
  To migrate your app traffic to a cluster using inter-node TLS, see <a href="#migration">Migrating to a Secure Inter-node Cluster</a> below.
</p>

##<a id="migration"></a> Migrate to a Secure Inter-node Cluster

<%# when do you have to do this procedure? is it mandatory to do it after configuring inter-node in the tile above? %>

This procedure only applies if you have an old <%= vars.product_short %>
service instance that you want to migrate from, with producer and consumer apps
communicating with that instance.

An Erlang distribution cannot contain a mix of nodes that communicate over TLS
and nodes that do not.
You cannot switch between the two while carrying out a rolling upgrade, you
must manually migrate between them.
This migration can involve less downtime if you use a blue-green deployment.

The following instructions assume that you are familiar with RabbitMQ concepts,
such as dynamic shovels and how to configure them.
If you need help with this process, contact a VMware Support representative.

<p class="note">
  <strong>Note:</strong> This process requires you to manually create RabbitMQ
  objects. This might not be feasible if you have more than 100 queues.
  VMware Support can provide a script to perform some of these steps if needed.
</p>

To migrate to a secure inter-node cluster:

1. In the subnet of your original service instance, create a new service instance
with the same plan and configuration as the original service instance.
For example, if you created the blue instance with TLS enabled, you could run:

    <pre class="terminal">
    $ cf create-service p.rabbitmq my-rabbitmq-plan rabbitmq-green -c '{"tls": true}'
    </pre>

    In this procedure, this new service instance is referred to as green
    and the old service instance is referred to as blue.

1. After the green service instance is deployed, create admin credentials on both the blue
and green clusters if they do not already have credentials:

    ```
    cf create-service-key rabbitmq-blue admin-key -c '{"tags": "administrator"}'
    cf create-service-key rabbitmq-green admin-key -c '{"tags": "administrator"}'
    ```
    <%# could rabbitmq-blue and rabbitmq-green be placehoders? %>

1. Log in to the Management UI on both the blue and green clusters.
You can find both the URL for the Management UI, as well as the admin credentials,
in the service-keys you just created:

    ```
    cf service-key rabbitmq-blue admin-key
    ```
    <%# could rabbitmq-blue be a placehoder? %>
    Make note of the AMQP(S) URI listed in the service-key,
    you will need this later to set up shovels between the two clusters.
    This is stored under the key <code>uri</code> in the service-key.

1. Stop your apps so that no new messages are sent to the blue cluster and no
new topology objects, such as queues or policies, are created.
Do so by running:

    ```
    cf stop APP-NAME
    ```
    <%# I changed this to a placehoder, is that OK? %>
    Where `APP-NAME` is the name of your messaging app.

1. In the **Overview** pane of the UI <%# is this the management UI? %> on the blue (old) cluster,
navigate to **Export Definitions** and click **Download broker definitions**.
This saves a JSON file to your local machine that contains the topology metadata you need to re-create
the cluster on another service instance.
  <p class="note">
    <strong>Note:</strong> If you do not see the <strong>Export definitions</strong> section of
    the <strong>Overview</strong> pane, it is likely that the service-key you created did not
    contain the tag to enable admin permissions.
    Re-check the command you ran to create the key.
  </p>

1. Modify the definitions file you downloaded in the previous step to remove
anything you do not want on your new cluster.
For example, the new cluster will create its own users, so you must remove any
user credentials before importing the file onto the new cluster.

1. In the **Overview** pane of the UI <%# is this the management UI? %> on the green (new) cluster,
navigate to **Import Definitions** and add the JSON file you downloaded from
the **blue** cluster and click **Upload broker definitions**.
This creates the topology required on the green cluster.
  <p class="note">
    <strong>Note:</strong> If you do not see the <strong>Import definitions</strong> section of
    the <strong>Overview</strong> pane, it is likely that the service-key you created did not
    contain the tag to enable admin permissions.
    Re-check the command you ran to create the key.
  </p>

1. Configure a shovel on the green cluster to drain any messages from the blue cluster.
You can do this in the Management UI of the green cluster as follows:

    1. Navigate to **Admin** and go to **Shovel Management** in the sidebar.
    1. Click **Add a new shovel**.
    1. Under **Source**:
        - Set the URI to the URI you retrieved from the service key for the blue cluster
     (except if you are using TLS for AMQP traffic, see Note below),
        - Set **Queue** to the name of a queue that you want to drain from the blue cluster.
        - Set **Auto-delete** to **After initial length transferred**. <%# are queue and auto-delete UI elements or fields? %>

    You must create one shovel for each queue that you want to drain to the green cluster.
    If you have many queues, VMware recommends creating these in batches and waiting for them to complete before creating
    new shovels, to avoid overloading the system.

    <p class="note">
      <strong>Note:</strong> If your clusters are using TLS for AMQP traffic,
    you must add a query parameter onto the end of your AMQPS URI when configuring the shovel.
    This query parameter is <strong>always</strong> <code>?cacertfile=/var/vcap/jobs/rabbitmq-server/etc/cacert.pem&certfile=/var/vcap/jobs/rabbitmq-server/etc/cert.pem&keyfile=/var/vcap/jobs/rabbitmq-server/etc/key.pem&verify=verify_peer</code>.
    For example, your shovel's source URI will<%# Avoid |will|: present tense is preferred. %> look similar to: <code>amqps://user:password@server-name?cacertfile=/var/vcap/jobs/rabbitmq-server/etc/cacert.pem&certfile=/var/vcap/jobs/rabbitmq-server/etc/cert.pem&keyfile=/var/vcap/jobs/rabbitmq-server/etc/key.pem&verify=verify_peer</code>
  </p>

1. Wait for all of the shovels to disappear from the Management UI of the green cluster.
This indicates that all messages have been drained from the blue cluster.

1. Unbind your apps from the blue cluster and bind them to the green cluster.
How you do this depends on if your apps are running in the same foundation as the service instance.
    - If your apps are running on the same foundation, re-bind your apps by running:

        ```
        cf unbind-service APP-NAME rabbitmq-blue
        cf bind-service APP-NAME rabbitmq-green
        ```
        <%# could rabbitmq-blue and rabbitmq-green be placehoders? %>

    - If your apps are running off-foundation and communicate with RabbitMQ through
    the Service Gateway, create a new service key on the green cluster by running:

        ```
        cf create-service-key rabbitmq-green messaging-app-key
        ```
        <%# could rabbitmq-green be a placehoder? %>

        Supply the service key to your app so that it can communicate with the new cluster.

1. Restart your apps. If you know which of your apps are consumers and which are producers,
VMware recommends that you restart the consumers first to avoid building up a backlog on the green cluster.

    ```
    cf start APP-NAME
    ```
    <%# I changed this to a placehoder, is that OK? %>

1. Confirm in the Management UI for the blue cluster that the blue cluster has
no messages in queues, no throughput.

1. Confirm in the Management UI for the green cluster that the green cluster has
the expected throughput.

1. After confirming the green cluster is as expected, delete the blue cluster by running:

    ```
    cf delete-service rabbitmq-blue
    ```
    <%# could rabbitmq-blue be a placehoder? %>
