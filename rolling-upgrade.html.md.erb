---
title: Rolling Upgrades in RabbitMQ for Pivotal Platform
owner: London Services
---

## <a id='overview'></a> Overview
A rolling upgrade is a strategy for updating a distributed system.

In a rolling upgrade on <%= vars.platform_name %>, each VM is updated in turn.
After the update completes, the VM is started, and after the specified processes are running, the update
procedure begins for the next VM in the sequence.

In a <%= vars.product_short %> cluster, each node runs on a separate VM.
Rolling upgrades help to ensure availability by keeping at least one node up throughout the upgrade process.

Before 1.17.3, some upgrades required the whole cluster to be shut down.
For example, when the major or minor version of <%= vars.product_short %> was updated, or when the major
version of the Erlang distribution was updated.

As of 1.17.3, upgrades are now performed using a rolling upgrade strategy.
The only case where a cluster is required to fully shut down as part of an upgrade is where the Erlang cookie
for the cluster is changed.
Development of <%= vars.product_short %> is on-going.
Downtime might occur when applying later versions of <%= vars.product_short %> to existing clusters.


## <a id='rolling'></a> Running a Rolling Upgrade for <%= vars.product_full %>
On a single canary node in the cluster, the following steps are carried out:
 - The <%= vars.product_short %> server process and the Erlang VM are stopped by running
`rabbitmqctl stop`
 - Persistent disk is detached
 - The VM is torn down for the node
 - After 0â€“5 seconds the BOSH DNS service detects a failing health check for
 the node that has just gone down, and no longer routes service traffic to the node
 - BOSH requests a new VM from the underlying IaaS, and attaches the persistent
 disk from the old VM
 - rabbitmq-server is started on the new VM, and the node rejoins the cluster
 - The new node registers with the BOSH DNS service and begins receiving traffic. The above steps are then
 carried out on the remaining nodes in the cluster, one by one.

## <a id='example'></a> Example Scenario
The following is an example of an upgrade scenario where rolling upgrade is employed.
An operator upgrades their platform to use a new version of <%= vars.product_full %>; they upgrade from
v1.17.4 to v1.18.1 of the product.
This example is designed to show a system performing a rolling upgrade under a heavy load, with substantial
disk I/O, with both the underlying <%= vars.product_short %> and Erlang software upgrading to a newer version.
Without a rolling upgrade, the whole cluster is required to shut down, resulting in a service outage given
publishers and consumers are unable to connect to the cluster.
Pivotal aims to explore the extent of downtime associated with a rolling upgrade in this case.

<p class='note'>
  <strong>Note:</strong> The following is provided for example purposes only and is not intended to represent
  all upgrade situations. Your platform setup might have different results.
</p>

### <a id='setup'></a> Configuration and Setup

Details of the configuration and setup of the experiment are detailed in the sections below.

#### <a id='cluster-config'></a> Cluster Configuration
The IaaS used for this experiment was Google Cloud Platform (GCP).
The <%= vars.product_short %> node VMs were configured with 2 CPUs, 2&nbsp;GB of RAM, 8&nbsp;GB of disk, and
5&nbsp;GB persistent disk.
Initially <%= vars.product_full %> v1.17.4 was installed with a plan configured to build a three-node
cluster with queues being mirrored.
This environment was then upgraded to use <%= vars.product_full %> v1.18.1.

#### <a id='app-config'></a> App Configuration

The RabbitMQ Performance Tool for Cloud Foundry was used to simulate the workload on the cluster.
This is a Java application that tests throughput of <%= vars.product_short %>.
For more information, see the
[RabbitMQ PerfTest for Cloud Foundry](https://github.com/rabbitmq/rabbitmq-perf-test-for-cf) repository in GitHub.

This tool uses a resilient client with reconnection and retry logic.
When the performance test is run it creates a direct exchange and a queue.
For more information, see the [RabbitMQ documentation](https://www.rabbitmq.com/tutorials/amqp-concepts.html).

In addition, it creates all the necessary consumers and producers and binds them to the newly created queue.
In our scenario the performance test is configured to use durable and mirrored queues, and persistent messages
which ensure that messages are persisted to disk.
For more information about durability, see the
[RabbitMQ documentation](https://www.rabbitmq.com/queues.html#durability).
For more information about mirrored queues, see the
[RabbitMQ documentation](https://www.rabbitmq.com/ha.html).

The protocol extension publisher confirms is enabled to ensure that there is no data loss.
For more information about publisher confirms, see the
[RabbitMQ documentation](https://www.rabbitmq.com/confirms.html#publisher-confirms).

This setup ensures that there is a backlog of message to be read from disk and consumed at any point during the
upgrade.

The publishers are configured to constantly produce messages in three different bursts:
1. 500 messages per second for 30 seconds
1. 750 messages per second for 15 seconds
1. 250 messages per second for 15 seconds

On the other hand the publishers are expected to consume a total of 500 messages per second.
Each message is a JSON blob of size 50000 bytes.

The equivalent app manifest for this test is as follows:
```yaml
---
applications:
  - name: rabbitmq-perf-test
    path: ./target/pcf-perf-test-1.0-SNAPSHOT.jar
    buildpacks:
      - https://github.com/cloudfoundry/java-buildpack.git
    memory: 2G
    health-check-type: process
    services: [rmq]
    env:
      VARIABLE_RATE: "500:30,750:15,250:15"
      CONSUMER_RATE: 500
      JSON_BODY: true
      SIZE: 50000
      SLOW_START: true
      METRICS_PROMETHEUS: true
      FLAG: persistent
      CONFIRM: 30000
```

### <a id='observe'></a> Observations

Tests show that downtime experienced during this rolling upgrade is significantly reduced compared to a similar
upgrade where the cluster is brought fully down.
Our metrics indicated that the downtime, in this case a publisher being unable to publish a message to a queue,
was 5 seconds at maximum.
This is because the internal BOSH DNS record used to round-robin messages to the nodes in the cluster has a TTL
of 5 seconds.
The messages can be routed to nodes which have just been replaced.
Because the tested app has retry logic, no service outage was observed.

There is more information about creating resilient apps in the
[workloads repository](https://github.com/rabbitmq/workloads/tree/master/resiliency) in GitHub.

Longer downtime is expected for a cluster under a greater load.
When a node comes back up and rejoins the cluster, messages from the other nodes sync with the newly joined node.
Queues on the newly joined node reject publishers and consumers until the messages are synced.

### <a id='consider'></a> Cluster Configuration Considerations

There would be downtime for a cluster without mirrored queues.
This is because when the hosting node is down, the queue does not exist and any published messages are dropped
unless the publisher uses the `mandatory` flag or the exchange is configured with an alternate exchange.
