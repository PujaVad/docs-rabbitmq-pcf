---
title: Monitoring and KPIs for VMware Tanzu RabbitMQ for VMs
owner: London Services
---
This topic explains how to monitor the health of the <%= vars.product_full %>
service using the logs, metrics, and Key Performance Indicators (KPIs) generated by
<%= vars.product_short %> component VMs.

This guide assumes you are running with service instances on RabbitMQ v3.9.x. The majority of the following metrics are
available on RabbitMQ v3.8.x, however VMware recommends upgrading to the latest version to take full advantage of
the observability features detailed in this doc.

<p class="note">
  <strong>Note:</strong> As of <%= vars.product_short %> v2.0, RabbitMQ Server metrics are now provided by the `rabbitmq_prometheus` plugin.
  Consequently, many metric names will change when upgrading to <%= vars.product_short %> v2.0.
  Both On-Demand <%= vars.product_short %> and Pre-Provisioned <%= vars.product_short %> are affected by this.
  For a list of the changes made in v2.0 to metric names, see [the migration guide](./migrate-2.0-metrics.html).
</p>

## <a id="metrics"></a> Metrics

Metrics are regularly-collected log entries that report measured component states.
Metrics may be consumed either through the Loggregator subsystem, which collects metrics automatically based on the metrics
polling interval, or you may configure a Prometheus server or Healthwatch tile to directly scrape VMs deployed by
<%= vars.product_short %>. The RabbitMQ servers themselves expose the same information in both cases.

For a full list of all metrics exposed in pre-provisioned and on-demand service instances of <%= vars.product_short %>,
see the [Component Metric Reference](#reference).

<p class="note">
  <strong>Note:</strong> As of <%= vars.product_short %> v2.0, the format of the metrics has changed.
  For a list of the changes made in v2.0 to metric names, see [the migration guide](./migrate-2.0-metrics.html).
</p>

### <a id="loggregator"></a> Collecting metrics with the Loggregator system

Loggregator-collected metrics are long, single lines of text that follow the format:

```mac
origin:"p-rabbitmq" eventType:ValueMetric timestamp:1616427704616569016 deployment:"cf-rabbitmq" job:"rabbitmq-broker" index:"0" ip:"10.0.4.101" tags:<key:"instance_id" value:"d4b4fd51-50de-4227-a96f-8ce636960f0b" > tags:<key:"source_id" value:"rabbitmq-broker" > valueMetric:<name:"_p_rabbitmq_service_broker_heartbeat" value:1 unit:"boolean" >
```
Provided the prometheus plugin is enabled, <%= vars.product_short %> will automatically collect these metrics for you
and forward them to the Loggregator system. See [Overview of Logging and Metrics](https://docs.pivotal.io/application-service/loggregator/data-sources.html)
for general information about logging and metrics in <%= vars.app_runtime_full %>, and how to consume the metrics
from the Loggregator system.

#### <a id="metrics-polling-interval"></a> Configure the Metrics Polling Interval

The metrics polling interval for Loggregator defaults to 30 seconds.
The **metrics polling interval** is a configuration option on the <%= vars.product_short %> tile
(**Settings** > **RabbitMQ**). Setting this interval to -1 disables metrics.
The interval setting applies to all components deployed by the tile.

To configure the metrics polling interval:

1. From the <%= vars.ops_manager %> Installation Dashboard, click the <%= vars.product_short %> tile.
1. In the <%= vars.product_short %> tile, click the **Settings** tab.
1. Click **Metrics**.
   ![Screenshot of the RabbitMQ tile with header
   'Metrics settings for both Pre-Provisioned and On-Demand service offerings' with one field: required
   text field, 'Metrics polling interval' with entered value 30 and help text, 'Select the polling
   interval for the RabbitMQ service metrics in seconds. Setting this field to -1 disabled metrics.'
   A blue 'Save' button is at the bottom of the page.](images/metrics-configuration-1.20.png)

1. Configure the fields on the **Metrics** pane as follows:
<table class="nice">
    <th>Field</th>
    <th>Description</th>
    <tr>
        <td><strong>Metrics polling interval</strong></td>
        <td>The default setting is 30 seconds for all deployed components.
          VMware recommends that you do not change this interval.
          To avoid overwhelming components, do not set this below 10 seconds.
          Set this to -1 to disable metrics.
          Changing this setting affects all deployed instances.</td>
    </tr>
</table>

1. Click **Save**.
1. Return to the <%= vars.ops_manager %> Installation Dashboard.
1. Click **Review Pending Changes**.
   For more information about this <%= vars.ops_manager %> page,
   see [Reviewing Pending Product Changes](https://docs.pivotal.io/pivotalcf/customizing/review-pending-changes.html).
1. Click **Apply Changes** to redeploy with the changes.

#### <a id="detailed-metrics"></a> Gather additional detailed metrics

In addition to the standard RabbitMQ server metrics gathered by <%= vars.product_short %>, it is possible to gather
additional, [detailed metrics](https://github.com/rabbitmq/rabbitmq-server/tree/master/deps/rabbitmq_prometheus#selective-querying-of-per-object-metrics)
for your system. You can elect for these additional metrics to be gathered only for specific vhosts, or for only
a subset of these metrics to be generated, in order to limit the performance impact of gathering more data.

The process to configure this differs for the different service offerings; see the instructions
[for on-demand](./use.html#detailed-metrics) and [for pre-provisioned](./install-config-pp.html#detailed-metrics)
for more info.

### <a id="prometheus"></a> Collecting metrics with Prometheus

Prometheus-style metrics are available at `SERVICE-INSTANCE-ID:15692/metrics`.
To pull these metrics from the service instances, you must deploy and configure a Prometheus instance.
For more information about the plugin and monitoring RabbitMQ using Prometheus and Grafana, see the
[RabbitMQ documentation](https://www.rabbitmq.com/prometheus.html).

The following Prometheus scrape config dynamically discovers RabbitMQ instances:

```
job_name: rabbitmq
metrics_path: "/metrics"
scheme: http
dns_sd_configs:
- names:
    - q-s4.rabbitmq-server.*.*.bosh.
  type: A
  port: 15692
```

The regular expression in the scrape config name ensures that Prometheus discovers all future service
instances too. Note that if you are using TLS in the on-demand service offering, your port will instead be 15691.

If Prometheus is deployed with the Healthwatch v2 tile, then the above configuration is automatically applied.

<p class="note">
  <strong>Note:</strong> By default, metrics are aggregated.
  This results in a lower performance overhead at the cost of lower data fidelity.
  For more information, see the
  <a href="https://www.rabbitmq.com/prometheus.html#metric-aggregation">RabbitMQ documentation</a>.
</p>


#### <a id="per-object"></a> Scrape Per-Object Metrics

To collect metrics on a per-object scope, such as per-queue, do one of the following:

- Enable per-object metrics by setting `prometheus.return_per_object_metrics = true`.
  For instructions, see [Expert mode: Overriding RabbitMQ Server configuration](./expert-override-config.html).

- Scrape the dedicated per-object metrics endpoint, for example:

    ```
    job_name: rabbitmq
    metrics_path: "/metrics/per-object"
    scheme: http
    dns_sd_configs:
    - names:
        - q-s4.rabbitmq-server.*.*.bosh.
      type: A
      port: 15692
    ```

<p class="note">
  <strong>Note:</strong> Collecting per-object metrics on a system with many objects,
  such as queues or connections, is very slow.
  Ensure you understand the impact on your system and its load before enabling
  this on a production cluster.
</p>

#### <a id="filter-per-object"></a> Filter the Per-Object Metrics

As of <%= vars.product_short %> v2.0.7, you can collect only the per-object metrics for
certain scopes of metrics.
This decreases the performance overhead, while retaining data fidelity for metrics that you are interested in.
For more information, see [Selective querying of per-object metrics](https://github.com/rabbitmq/rabbitmq-server/tree/master/deps/rabbitmq_prometheus#selective-querying-of-per-object-metrics).

For example, the following scrape config collects only the per-object metrics that allow you to see how
many messages sit in every queue and how many consumers each of these queues have:

```
job_name: rabbitmq
metrics_path: "/metrics/detailed?family=queue_coarse_metrics&family=queue_consumer_count"
scheme: http
dns_sd_configs:
- names:
    - q-s4.rabbitmq-server.*.*.bosh.
  type: A
  port: 15692
```

### <a id="Grafana"></a> Grafana Dashboards

The RabbitMQ team has written dashboards that you can import into Grafana.
These dashboards include documentation for each metric.

* **[RabbitMQ-Overview](https://grafana.com/grafana/dashboards/10991):**
  Dashboard for an overview of the RabbitMQ system

* **[Erlang-Distribution](https://grafana.com/grafana/dashboards/11352):**
  Dashboard for the underlying Erlang distribution

For more information about these dashboards, see the
[RabbitMQ documentation](https://www.rabbitmq.com/prometheus.html).
If Grafana is deployed using the Healthwatch v2 tile, you can load these dashboards by selecting the
**Enable RabbitMQ dashboards** checkbox in the Healthwatch tile.

### <a id="heartbeats"></a> Component Heartbeats

Some components periodically emit Boolean heartbeat metrics to the Loggregator system.
<code>1</code> means the system is available, and <code>0</code> or the absence of a heartbeat metric
means the service is not responding and should be investigated.

#### <a id="broker-heartbeat"></a> Service Broker Heartbeat

<table>
   <tr><th colspan="2" style="text-align: center;"><br>_p_rabbitmq_service_broker_heartbeat<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>RabbitMQ Service Broker <code>is alive</code> poll, which indicates if the component is
        available and able to respond to requests.<br><br>

      <strong>Use</strong>: If the Service Broker does not emit heartbeats, this indicates that it
      is offline.
      The Service Broker is required to create, update, and delete service instances, which are
       critical for dependent tiles such as Spring Cloud Services and Spring Cloud Data Flow.
      <br><br>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: boolean<br>
      <strong>Frequency</strong>: 30 s (default), 10 s (configurable minimum)<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: N/A<br>
      <strong>Red critical</strong>: &lt; 1</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
        Search the RabbitMQ Service Broker logs for errors.
        You can find this VM by targeting your <%= vars.product_short %> deployment with BOSH and
        running the command.
               <code>bosh -d service-instance_GUID vms</code> (On-demand)
               <code>bosh -d p-rabbitmq-GUID vms</code> (Pre-provisioned)
      </td>
   </tr>
</table>

##### <a id="haproxy-heartbeat"></a> HAProxy Heartbeat
<p class="note">
  <strong>Note:</strong> HAProxy is only used in the pre-provisioned service offering, and so heartbeats will
  only be present if this service offering is enabled.
</p>

<table>
   <tr><th colspan="2" style="text-align: center;"><br> _p_rabbitmq_haproxy_heartbeat<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>RabbitMQ HAProxy <code>is alive</code> poll, which indicates if the component is available and
          able to respond to requests.<br><br>

      <strong>Use</strong>: If the HAProxy does not emit heartbeats, this indicates that it is offline. To be functional, service instances require HAProxy.
      <br><br>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: boolean<br>
      <strong>Frequency</strong>: 30 s (default), 10 s (configurable minimum)<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: N/A<br>
      <strong>Red critical</strong>: &lt; 1</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
         Search the RabbitMQ HAProxy logs for errors.
         You can find the VM by targeting your RabbitMQ deployment with BOSH and
         running the following command, which lists <code>HAProxy_GUID</code>:<br>
         <code>bosh -d service-instance_GUID vms</code>
      </td>
   </tr>
</table>


### <a id="kpi"></a> Key Performance Indicators

The following sections describe the metrics used as Key Performance Indicators and other useful
metrics for monitoring the <%= vars.product_short %> service.

Key Performance Indicators (KPIs) for <%= vars.product_short %> are metrics that operators find most
useful for monitoring their <%= vars.product_short %> service to ensure smooth operation.
KPIs are high-signal-value metrics that can indicate emerging issues.
KPIs can be raw component metrics or derived metrics generated by applying formulas to raw metrics.

VMware provides the following KPIs as general alerting and response guidance for typical
<%= vars.product_short %> installations.
VMware recommends that operators continue to fine-tune the alert measures to their installation by
observing historical trends.
VMware also recommends that operators expand beyond this guidance and create new,
installation-specific monitoring metrics, thresholds, and alerts based on learning from their own
installations.

For a list of all <%= vars.product_short %> raw component metrics, see
[Component Metrics Reference](#reference) below.

#### <a id="kpi-heartbeat"></a> Component Heartbeats
If collecting metrics through loggregator, several components in <%= vars.product_short %> emit heartbeat
metrics. For more information, see [Component Heartbeats](#heartbeats).

#### <a id="file-descriptors"></a> RabbitMQ Server File Descriptors

<table>
   <tr><th colspan="2" style="text-align: center;"><br> rabbitmq_process_open_fds<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>File descriptors consumed.<br><br>

      <strong>Use</strong>: If the number of file descriptors consumed becomes too large,
              the VM might lose the ability to perform disk I/O, which can cause data loss.
      <p class="note"><strong>Note:</strong> This assumes non-persistent messages are handled by
        retries or some other logic by the producers.</p>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: count<br>
      <strong>Frequency</strong>: 30 s (default), 10 s (configurable minimum)<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 10 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &gt; 250000 <br>
      <strong>Red critical</strong>: &gt; 280000</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>The default <code>ulimit</code> for <%= vars.product_short %> is 300000.
        If this metric is met or exceeded for an extended period of time, consider reducing the load
        on the server.</td>
   </tr>
</table>


#### <a id="erlang-processes"></a> Erlang Processes

<table>
   <tr><th colspan="2" style="text-align: center;"><br> erlang_vm_process_count<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td><a href="https://www.erlang.org/docs">Erlang</a> processes consumed by RabbitMQ, which runs on an Erlang VM.<br><br>

      <strong>Use</strong>: This is the key indicator of the processing capability of a node.
      <br><br>
      <strong>Origin</strong>: Doppler/Firehose<br>
      <strong>Type</strong>: count<br>
      <strong>Frequency</strong>: 30 s (default), 10 s (configurable minimum)<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 10 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &gt; 900000 <br>
      <strong>Red critical</strong>: &gt; 950000</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>The default Erlang process limit in <%= vars.product_short %> v1.6 and later is 1,048,816.
        If this metric meets or exceeds the recommended thresholds for extended periods of time,
        consider scaling the RabbitMQ nodes in the tile <b>Resource Config</b> pane.
      </td>
   </tr>
</table>


### <a id="bosh"></a> BOSH System Health Metrics

<%# The below partial is in https://github.com/pivotal-cf/docs-partials %>

<%= partial vars.path_to_partials + '/services/bosh_health_metrics_pcf2' %>

All BOSH-deployed components generate the system health metrics below.
These component metrics are from <%= vars.product_short %> components, and serve as KPIs for
the <%= vars.product_short %> service.

#### <a id="ram"></a> RAM

<table>
   <tr><th colspan="2" style="text-align: center;"><br> system_mem_percent <br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>RAM being consumed by the <code>p.rabbitmq</code> VM.<br><br>

      <strong>Use</strong>: RabbitMQ is considered to be in a good state when it has few or no messages.
              In other words, "an empty rabbit is a happy rabbit."
              Alerting on this metric can indicate that there are too few consumers or apps that
              read messages from the queue.
      <br><br>
      Healthmonitor reports when RabbitMQ uses more than 40% of its RAM for the past ten minutes.
      <br><br>
      <strong>Origin</strong>:  BOSH HM<br>
      <strong>Type</strong>: percent<br>
      <strong>Frequency</strong>: 30 s (default), 10 s (configurable minimum)<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 10 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &gt; 40 <br>
      <strong>Red critical</strong>: &gt; 50</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>Add more consumers to drain the queue as fast as possible.
      </td>
   </tr>
</table>

#### <a id="cpu"></a> CPU

<table>
   <tr><th colspan="2" style="text-align: center;"><br> system_cpu_user<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>CPU being consumed by user processes on the <code>p.rabbitmq</code> VM.<br><br>

      <strong>Use</strong>: A node that experiences context switching or high CPU usage becomes unresponsive.
      This also affects the ability of the node to report metrics.
      <br><br>
      Healthmonitor reports when RabbitMQ uses more than 40% of its CPU for the past ten minutes.
      <br><br>
      <strong>Origin</strong>:  BOSH HM<br>
      <strong>Type</strong>: percent<br>
      <strong>Frequency</strong>: 30 s (default), 10 s (configurable minimum)<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 10 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &gt; 60 <br>
      <strong>Red critical</strong>: &gt; 75</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td> Remember that "an empty rabbit is a happy rabbit". Add more consumers to drain the queue as fast as possible.
      </td>
   </tr>
</table>

#### <a id="ephemeral-disk"></a> Ephemeral Disk

<table>
   <tr><th colspan="2" style="text-align: center;"><br> system_disk_ephemeral_percent<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Ephemeral Disk being consumed by the <code>p.rabbitmq</code> VM.<br><br>
      <strong>Use</strong>: If system disk fills up, there are too few consumers.
      <br><br>
      Healthmonitor reports when RabbitMQ uses more than 50% of its Ephemeral Disk for the past ten minutes.
      <br><br>
      <strong>Origin</strong>:  BOSH HM<br>
      <strong>Type</strong>: percent<br>
      <strong>Frequency</strong>: 30 s (default), 10 s (configurable minimum)<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 10 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &gt; 50 <br>
      <strong>Red critical</strong>: &gt; 75</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>Remember that "an empty rabbit is a happy rabbit". Add more consumers to drain the queue as
        fast as possible. Insufficient disk space leads to node failures and might result in data
        loss due to all disk writes failing.
      </td>
   </tr>
</table>

#### <a id="persistent-disk"></a> Persistent Disk

<table>
   <tr><th colspan="2" style="text-align: center;"><br> system_disk_persistent_percent<br><br></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>Persistent Disk being consumed by the <code>p.rabbitmq</code> VM.<br><br>
      <strong>Use</strong>: If system disk fills up, there are too few consumers.
      <br><br>
      Healthmonitor reports when RabbitMQ uses more than 50% of its Persistent Disk.
      <br><br>
      <strong>Origin</strong>:  BOSH HM<br>
      <strong>Type</strong>: percent<br>
      <strong>Frequency</strong>: 30 s (default), 10 s (configurable minimum)<br>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 10 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &gt; 50 <br>
      <strong>Red critical</strong>: &gt; 75</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>Remember that "an empty rabbit is a happy rabbit". Add more consumers to drain the queue as fast as possible. Insufficient disk space leads to node failures and might result in data loss due to all disk writes failing.
      </td>
   </tr>
</table>

## <a id="logging"></a> Logging

You can configure <%= vars.product_short %> to forward logs to an external syslog server, and customise the format of
the logs output.

### <a id="syslog-forwarding"></a> Configure Syslog Forwarding

Syslog forwarding is preconfigured and enabled by default.
VMware recommends that you keep the default setting because it is good operational practice.
However, you can opt out by selecting **No** for **Do you want to configure syslog?** in the
<%= vars.ops_manager %> **Settings** tab.

To enable monitoring for <%= vars.product_short %>, operators designate an external syslog endpoint
for <%= vars.product_short %> component log entries.
The endpoint serves as the input to a monitoring platform such as Datadog, Papertrail, or SumoLogic.

To specify the destination for <%= vars.product_short %> log entries:

1. From the <%= vars.ops_manager %> Installation Dashboard, click the <%= vars.product_short %> tile.
1. In the <%= vars.product_short %> tile, click the **Settings** tab.
1. Click **Syslog**.
![Screenshot of RabbitMQ tile settings with header
called 'Syslog'. The page has several fields:
Radio button group, 'Do you want to configure Syslog forwarding?'
with two options: 'No, do not forward Syslog' or 'Yes'.
Required text field, 'Address',
required text field, 'Port' with value 22822,
required dropdown field, 'Transport Protocol' with TCP selected,
disabled checkbox, 'Enabled TLS',
grayed required text field, 'Permitted Peer'.
grayed required text area field, 'SSL Certificate',
text field, 'Queue Size',
checkbox field, 'Forward Debug Logs', and
textarea field, 'Custom rsyslog Configuration'.
A blue botton is at the bottom called 'Save Syslog Settings'.](images/syslog-config-1.20.0.png)
1. Configure the fields on the **Syslog** pane as follows:
<table class="nice">
    <th>Field</th>
    <th>Description</th>
    <tr>
        <td><strong>Syslog Address</strong></td>
        <td>Enter the IP or DNS address of the syslog server</td>
    </tr>
    <tr>
        <td><strong>Syslog Port</strong></td>
        <td>Enter the port of the syslog server</td>
    </tr>
    <tr>
        <td><strong>Transport Protocol</strong></td>
        <td>Select the transport protocol of the syslog server. The options are <strong>TLS</strong>,
          <strong>UDP</strong>, or <strong>RELP</strong>.</td>
    </tr>
    <tr>
        <td><strong>Enable TLS</strong></td>
        <td>Enable TLS to the syslog server.</td>
    </tr>
    <tr>
        <td><strong>Permitted Peer</strong></td>
        <td>If there are several peer servers that can respond to remote syslog connections,
            enter a wildcard in the domain, such as <code>*.example.com</code>.</td>
    </tr>
    <tr>
        <td><strong>SSL Certificate</strong></td>
        <td>If the server certificate is not signed by a known authority, such as an internal syslog
          server, enter the CA certificate of the log management service endpoint.</td>
    </tr>
    <tr>
        <td><strong>Queue Size</strong></td>
        <td>The number of log entries the buffer holds before dropping messages.
          A larger buffer size might overload the system. The default is 100000.</td>
    </tr>
    <tr>
        <td><strong>Forward Debug Logs</strong></td>
        <td>Some components produce very long debug logs. This option prevents them from being
          forwarded.
          These logs are still written to local disk.</td>
    </tr>
    <tr>
        <td><strong>Custom Rules</strong></td>
        <td>The custom rsyslog rules are written in
          <a href="https://www.rsyslog.com/doc/v8-stable/rainerscript/index.html">RainerScript</a>
          and are inserted before the rule that forwards logs.
          For the list of custom rules you can add in this field, see
          <a href="#rabbitmq-syslog-custom-rules">RabbitMQ Syslog Custom Rules</a> below.
          For more information about the program names you can use in the custom rules, see
          <a href="#program-names">Program Names</a> below.</td>
    </tr>
</table>

1. Click **Save**.
1. Return to the <%= vars.ops_manager %> Installation Dashboard.
1. Click **Review Pending Changes**.
For more information about this <%= vars.ops_manager %> page,
see [Reviewing Pending Product Changes](https://docs.pivotal.io/pivotalcf/customizing/review-pending-changes.html).
1. Click **Apply Changes** to redeploy with the changes.


### <a id="log-format"></a> Logging Format

With <%= vars.product_short %> logging configured, several types of components generate logs:
the RabbitMQ message server nodes, the service brokers, and (if present) HAProxy.

* The logs for RabbitMQ server nodes follow the format <code>[job:"rabbitmq-server" ip:"192.0.2.0"]</code>
* The logs for the pre-provisioned RabbitMQ service broker follow the format <code>[job:"rabbitmq-broker" ip:"192.0.2.1"]</code>
* The logs for the on-demand RabbitMQ service broker follow the format <code>[job:"on-demand-broker" ip:"192.0.2.2"]</code>
* The logs for HAProxy nodes follow the format <code>[job:"rabbitmq-haproxy" ip:"192.0.2.3"]</code>

RabbitMQ and HAProxy servers log at the <code>info</code> level and capture errors, warnings, and informational messages.

<%= partial vars.path_to_partials + '/rabbitmq/log-formats' %>

## <a id="reference"></a> Component Metrics Reference

<%= vars.product_short %> component VMs emit the following raw metrics.

<p class="note">
  <strong>Note:</strong> As of <%= vars.product_short %> v2.0, the format of the metrics has changed.
  For a list of the changes made in v2.0 to metric names, see [the migration guide](./migrate-2.0-metrics.html).
</p>

### <a id="rabbitmq-metrics"></a> RabbitMQ Server Metrics

RabbitMQ server metrics are emitted by the `rabbitmq_prometheus` plugin. The list of metrics provided is extensive,
and allows full observability of your messages, VM health and more.

For the full list of metrics emitted, see the
[rabbitmq-server](https://github.com/rabbitmq/rabbitmq-server/blob/master/deps/rabbitmq_prometheus/metrics.md)
repository in GitHub.

### <a id="haproxy-metrics"></a>HAProxy Metrics (Pre-provisioned only)

<%= vars.product_short %> HAProxy components emit the following metrics.

<table>
    <tr>
        <th>Name Space</th>
        <th>Unit</th>
        <th>Description</th>
    </tr>
    <tr>
        <td><code>_p_rabbitmq_haproxy_heartbeat</code></td>
        <td>boolean</td>
        <td>Indicates whether the RabbitMQ HAProxy component is available and able to respond to requests</td>
    </tr>
    <tr>
        <td><code>_p_rabbitmq_haproxy_health_connections</code></td>
        <td>count</td>
        <td>The total number of concurrent front-end connections to the server</td>
    </tr>
    <tr>
        <td><code>_p_rabbitmq_haproxy_backend_qsize_amqp</code></td>
        <td>size</td>
        <td>The total size of the AMQP queue on the server</td>
    </tr>
    <tr>
        <td><code>_p_rabbitmq_haproxy_backend_retries_amqp</code></td>
        <td>count</td>
        <td>The number of AMQP retries to the server</td>
    </tr>
    <tr>
        <td><code>_p_rabbitmq_haproxy_backend_ctime_amqp</code></td>
        <td>time</td>
        <td>The total time to establish the TCP AMQP connection to the server</td>
    </tr>
</table>

### <a id="odb-metrics"></a>On-demand Broker Metrics (On-demand only)

The <%= vars.product_short %> on-demand broker emits the following metrics.

<table>
    <tr>
        <th>Name Space</th>
        <th>Unit</th>
        <th>Description</th>
    </tr>
    <tr>
        <td><code>_on_demand_broker_p_rabbitmq_quota_remaining</code></td>
        <td>count</td>
        <td>The total quota for on-demand service instances set for this broker</td>
    </tr>
    <tr>
        <td><code>_on_demand_broker_p_rabbitmq_total_instances</code></td>
        <td>count</td>
        <td>The total count of on-demand service instances created by this broker</td>
    </tr>
    <tr>
        <td><code>_on_demand_broker_p_rabbitmq_{PLAN_NAME}_quota_remaining</code></td>
        <td>count</td>
        <td>The total quota for on-demand service instances set for this broker for a specific plan</td>
    </tr>
    <tr>
        <td><code>_on_demand_broker_p_rabbitmq_{PLAN_NAME}_total_instances</code></td>
        <td>count</td>
        <td>The total count of on-demand service instances created by this broker for a specific plan</td>
    </tr>
</table>
